Algorithm to separate text blobs from non-text blobs before running OCR.
Hello everyone, I am a python noob (been doing some projects for the past 3 months) and I decided that for this project I would dive into CV because it seems interesting af. I am trying to get a program to read a document and spit out an excel file with its contents. The first thing I tried was to simply plug the whole thing into a commercial OCR program (Abby Finereader), but clearly discovered that would not give me the desired results. I then understood I had to divide the problem into smaller, more manageable problems. So far I have divided the document into blobs of canny edges and then grouped the blobs using DBSCAN. I then computed bounding boxes for the clusters and cropped the original image into many smaller images. Some of these resulting "Pieces" have text and others have pictures. Before feeding the files in the folder into OCR, I would like to make sure i'm only giving it text-like looking images. I have tried several deterministic criteria to separate the images, but none is singularly robust. P.S. I have the images in pure bw for this step of analysis. First, I tried simply counting the number of black pixels per line and averaging them. Images with more than x or less than y average black pixels per line were removed. This got rid of the darkest and whitest pictures, but some "in-between" pictures remained.  I then thought about counting the average number of color switches in each row. It seemed to me that text's vertical nature would lead to a very high number of per-line color switches (b-w-w-b-b-b-w-w-w) ect. However, some images also produce similar number of switches.  I then tried using canny edges and computing the edge/total pixel ratio for every piece. This just seemed like a refinement of idea #2, but produced very similar results.   All in all, I have had fun pondering what makes text look like text in a deterministic sense, but I am currently faced with a fork in the road and I thought people here with experience could tell me what approach is more robust. First, I could use a combination of these types of deterministic rules and other ones i can conceive (like some sort of radial test,ect.) with more relaxed thresholds so that the combination of all of them yields more robust results than any one individually. However, this is a lot of work because the combinations of thresholds might require a lot of fine-tuning and it might not generalize nicely to other documents. Second, I could turn this into a classification problem and use some ML approach to solving it. I am well-versed in stats, but not much in ML. I guess I could create a training set and train a VSM ? I don't understand how the input data has to look like for ML, do I just feed it the raw image as an array? or do I feed it some data computed from the image's array ? Thanks a lot for your help. I really find the problem fascinating and am just looking for some pointers in the right direction. Some Examples of What Images Im dealing With  submitted by /u/SacoETrampa [link] [comments] 