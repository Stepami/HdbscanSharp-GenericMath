Gaussian Activation Functions?
I'm very out of the loop when it comes to machine learning, so please bear with me if I'm being totally stupid or naive here. When looking at tutorials for machine learning it's common to see activation functions that start at 0, then flip over to 1 like sigmoid or relu (even if that doesn't go from 0 to 1). I've been, learning/read some articles about how people make RNNs where you have one hot vectors as an input got me thinking about why exactly you have to have the one hot vector instead of just putting the number of the character in. It seems like, to my uninformed perspective, that a layer with a 0 then 1 sort of function is going to not be able to "pick' a neuron to activate. You can't have a range where X activates, then a range where Y activates, then a range where Z activates, because if Y has a higher weight than X then if Y is getting a large enough input from a single neuron, then X will also get a high enough input. But it also seems like a "gaussian" activation function that starts low, goes high, then goes back low would allow that sort of behavior where you can just put "character" as a single input into a network and let the network learn which neurons will activate for which input values, instead of needing to use a one hot vector. I'm guessing something is very wrong with my reasoning here, and was wondering if you all had any input or thoughts on why this isn't done?  submitted by /u/reguile [link] [comments] 