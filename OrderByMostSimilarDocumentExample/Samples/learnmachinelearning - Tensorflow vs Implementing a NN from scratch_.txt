Tensorflow vs Implementing a NN from scratch?
I was wondering what would be the optimal strategy to learn about neural networks. My goal, for now, is to deploy neural nets on problems and just tinker around with them. But I do realize the pitfalls of merely deploying a black box model (aka high-level library like Keras), without understanding the stuff under the hood. Plug-and-play with Keras is fun, but I don't think very fruitful in terms of learning. Here's my background: I can code up a logistic regression model from scratch, and as such, I'm pretty familiar with gradient descent, the cross-entropy loss function, and I can derive the backprop equations. So, for a simple logit model, I know what's going on under the hood. I realize coding up a neural net scratch involves the same concepts, just that it's more math-y. More layers, more equations, different activation function for each layer -- that's when my head starts to spin. I'm looking for that sweet spot where there's enough abstraction that I don't have to worry about manually computing gradients, but not so much that I have no idea what's really going on. How do I get there? Should I invest in fully understanding a NN? Or should I get started with a framework like tensorflow already?  submitted by /u/starstruck_21 [link] [comments] 