[D] Does a convolution layer decrease the input's dimensions like a pooling layer, or does it not?
I'm a bit perplexed about this. Let's say I have a 32x32x3 input (RGB image), and am about to use a CNN on it, with several Conv-Pool layers and one or two Fully Connected (dense-dropout) layers. I read this paper that uses Torch, and apparently its Conv layer reduces the size by kernel_size-1, so when the 32x32x3 image passes through the first Conv layer with 5x5 kernel size and 64 filters, it produces 28x28x64 segments/features. Whereas in Tensorflow, as far as I know, it doesn't, so when the 32x32x3 image passes through its Conv layer with 5x5 kernel size, and 64 filters it produces 32x32x64 features. Does it depend on the type of ML used (Caffe/Tensorflow/Torch/scikit-learn), and what effect does it have?  submitted by /u/FantasyBorderline [link] [comments] 