How to train a probability map with Image-level classes
I'm attempting to create a CNN model using binary image classes that can pick out regions from a large image. The concept behind this is to train a classifier, cut off the end, and use the new final layer as a map of activation regions. That output's shape is (n,n,1) to make it more interpretable. Most regions are non-discriminatory even in positive samples so I would like to be able to crop out the most discriminatory patches using this model after training. I'm having trouble aggregating the (n,n,1) space to a single value in a way that produces good maps. I have tried: GlobalMaxPooling- Works well with Negative Samples, but in positive samples the 4th highest positive region is typically < 0.5 Dropout on the map- doesn't help unless droprate is very high (.8-.95), and then negative samples struggle to learn, and training takes forever. Any-of-K-Trials- Something like (1-(1-n1)(1-n2)...), sometimes with the nth root on each factor proportional to the area. There's too much fine tuning, and inference totally vanishes, 0.4 can be the max value on negative samples, with 0.1 being the max in positive samples. Can anyone suggest a better way of aggregating these values so that i'm not training on the same patch(s) every epoch, while also not straying too far from the type of inference that global max pooling provides? Currently I'm thinking that it may be possible to regularize the activity of the (n,n,1) output with a contrast penalty or something, as adjacent pixels in the output overlap 75% on the input (r.f. width is 4x r.f. stride). Any advice or pointers are appreciated.  submitted by /u/vannak139 [link] [comments] 