XGBoost: quantile loss
Currently, I am using XGBoost for a particular regression problem. Instead of just having a single prediction as outcome, I now also require prediction intervals. Quantile regression with XGBoost would seem like the way to go, however, I am having trouble implementing this. I have already found this resource, but I am having trouble understanding it. I can see they are introducing an alternative to the standard quantile loss function, but I am having trouble interpreting the newly introduced parameters. After experimenting with this implementation for a bit, the model outcome seems to be super sensitive with respect to the parameter values. At the moment I don't even have a feel for the scale of these parameters, so extending it to my particular regression problem is difficult (although not impossible). I am also aware that sklearn allows for (standard) quantile regression with gradient boosted trees. It does solve the problem, but now I am curious as to how to implement this in XGBoost. If anyone can point me to a resource (that includes the mathematics), that would be greatly appreciated.  submitted by /u/aqmBE [link] [comments] 