[D] Information Theory 101 and how it helps to understand machine learning
Hey guys, I stumbled upon an article that I think will interest anyone discovering Machine Learning. Information Theory 101 This informative and let's be honest, quite dense post, goes back to the roots of information theory, giving the spotlight to Claude E. Shannon who layed out the basics of modern information theory in his paper Mathematical Theory of Communication published in 1948. Before Shanon, information theory was already studied but much different from what we know today. Essentially, most observations, rules and theorems were channel-specific. The only understanding everybody agreed upon was the tradeoff between rate and reliability. In this paper, Shanon proved the tradeoff invalid. Why should I care about Information Theory for Machine Learning? The goal of machine learning is to have a computer able to learn by himself, without being explictly programmed. So, to put it back in our terms, ML goals is to maximizes mutual information. Diverse Information Theory paradigm are now frequently used for Machine Learning such as: Clustering by Compression (ZIP/RAR/GIF/JPEG...): Instead of saving twice (or more) a redundant information, only the reference to the second iteration of the information (and the nth one) are saved to reduce the overall file size without loosing quality.  Classification: Both supervised and unsupervised classifications methods relie on paradigms coming from Information Theory. They are useful to sort large set of data: classifying a music library by genre for instance.   You can find the article here. It's quite a read but it's definitely worth it to anyone who wanna beef or freshen up one's knowledge on Information Theory and one's understanding of Machine Learning. So have you read it? What did you learn?  submitted by /u/RedditEpiphanySnail [link] [comments] 