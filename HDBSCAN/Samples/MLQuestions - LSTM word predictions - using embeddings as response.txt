LSTM word predictions - using embeddings as response
I've seen in lots of examples of the char-rnn example adapted for predicting the next word, the word embedding is used as the input for the sequence of words. However in all of these examples the author uses the one hot encoding of all tens of thousands of words as the response vector, and I'm wondering why this problem isn't framed with the embeddings as the response variable as well? You'd only need a couple hundred output dimensions in that case, and it seems like it would allow for more creativity in sampling words. What are the downsides to doing this?  submitted by /u/Cjh411 [link] [comments] 