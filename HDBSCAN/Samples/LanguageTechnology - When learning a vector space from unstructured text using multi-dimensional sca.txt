When learning a vector space from unstructured text using multi-dimensional scaling, should you remove infrequent words?
Hey, I'm wondering if I should be removing infrequent words (e.g. words that haven't occurred in at least 50 documents) from the 20 newsgroups dataset before putting them into a multi-dimensional scaling algorithm for dimensionality reduction. I haven't seen anyone mention removing infrequent terms in scientific papers as a preprocessing step, but in the scikit-learn package the CountVectorizer (that converts text into term:frequency pairings) has these parameters on by default. Should I be removing infrequent words? If so, how many is reasonable for a dataset like the 20 newsgroups dataset? Thanks for any help.  submitted by /u/ThomasAger [link] [comments] 