/u/CyberByte on Is there a need for the development of Strong AI?
Weak AI development seems to me to be a much better use of time and research money than the development of Strong AI  Well, you're in luck, because the vast majority of time and research money is in fact going towards weak AI. say we were to make a strong AI, what would we do with it or are we researching it in a 'just because we can' manner?  Different researchers have different motivations. I'm not sure "because we can" is a strong one (especially at the moment, because we can't), but "because I'm curious" probably is. It also depends on what you mean by "strong AI". This has often been associated with sentience (or phenomenal consciousness), which does not in and off itself have any functional component and is therefor not "useful" (as opposed to access consciousness). I think the main reason to want this (beyond curiosity) is because we value consciousness to some degree. If you could live forever by uploading your mind, you'd probably want to retain sentience. Or we might be more okay with (gradually and peacefully) going extinct if our more intelligent transhuman machine successors are sentient and can enjoy the universe in our place. If by "strong AI" you just mean "general AI" though, then this is one of the most powerful technologies imaginable. (Human) "general I" is responsible for all innovations in the history of our race. If we can somehow improve on that or even just scale it up, it should improve our ability to innovate and solve problems across the board. And if one of those problems is to improve AGI, we might get a recursive self-improvement fueled intelligence explosion, although the shape and speed of such a takeoff are currently unknown. Sure, we can build narrow AI to do many things (although probably not all things), but it still takes time and effort. And this is actually something you could use an AGI for. I sometimes hear people say that on particular tasks narrow beats general, but even when that's the case, an AGI may be able to create a better narrow AI than you or me if it's smarter or works tirelessly around the clock with a legion of AGI coworkers. There are definitely reasons to not want to build an AGI (see /r/ControlProblem), but there are also many reasons for people to want it (especially if they don't believe the risks). Whether we need it is a slightly different question. Did we need the internet? Well, no, but it's pretty awesome. Even if we're willing to forego most of the benefits of massively better technology, there's at least some change we might need it to survive: e.g. if we need much higher intelligence to avert other existential risks listed here.   