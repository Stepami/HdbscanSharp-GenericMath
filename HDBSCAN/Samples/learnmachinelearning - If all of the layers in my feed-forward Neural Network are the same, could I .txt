If all of the layers in my feed-forward Neural Network are the same, could I compute the derivative like so?
Would I be able to compute a general derivative for the weights coming into any layer based on the error of that layer's output (which comes from my error function)? Example: Doing that would result in something like Errors * sigmoid_prime(outputs) * inputs for a basic net that uses sigmoid activation and no biases. Technically, the Errors is the (target - actual) extracted from the error function by the chain rule. The output layer is easy, since we have target and actual work with. For the rest of the layers, we "backpropagate" the error by multiplying the transpose weights to figure out how much each node was to blame... and then plug that matrix into the Error part of the derivative equation. Is this procedure correct? Viable? Best practice? Thanks!  submitted by /u/ClearlyCoder [link] [comments] 