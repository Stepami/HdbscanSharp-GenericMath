[D] Suggestions on feeding very different 2D sensor inputs into a CNN
I'm running a regression model based on change over time in various 2D sensor inputs. Each sensor captures a 100x100 image and there are 5 sensors. 5 sensors x 2 time steps gives me 10 "channels". I know there are more sophisticated ways of handling the temporal dimension, but to start with, I've naively stacked the 10 channels and am feeding them into a single, very vanilla CNN (3 blocks of CNN layers feeding into a FC layer). I have two questions: Does it matter if for each sensor I stack the "before" and "after" values vs., say, the "before" and the "diff" (i.e., "after" - "before")? Would seem that the CNN can arrive at one from the other quite easily through the weights, so it shouldn't matter. (I don't see a major difference in my few experiments so far.)  There is a spatial relationship between the sensors inputs, but they are measuring very different things. So, it's interesting when features from two diff. sensors are in the same region of the 100x100 space, but the sensor data is "shaped" very differently. (For instance, RGB channels of animal pictures look very similar when you inspect them individually. But here, one sensor's input may be large, rounded blobs whereas another's may be fine stripes.) Is the naive stacking into one CNN a decent fit for this type of input? Will the CNN filters figure out mostly what to extract? Or do I need to be looking at something fancier like 3D cross-channel convolution?   Thanks.  submitted by /u/whydomyjointshurt [link] [comments] 