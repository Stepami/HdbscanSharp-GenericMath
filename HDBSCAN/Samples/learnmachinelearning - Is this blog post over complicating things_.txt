Is this blog post over complicating things?
Hey all, I'm reading through here: https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/ and the author takes the following approach to backprop: 1.) Calculate derivative for output layer error w.r.t hidden-output weights. 2.) Calculate derivative for output layer error w.r.t input-hidden weights. 3.) Plug in the numbers. Hmm, ok. So that's valid. But he never back propagated the errors like I'm used to. What I expected was: 1.) Calculate derivative for output layer w.r.t hidden-output weights. 2.) back propagate error by using the hidden-output weights to determine how much each hidden node is to blame. 3.) apply the same formula from step 1* to step 2 results, now that the hidden units have their own error Are these the same thing? Is mine actually wrong? Thanks * I guess I should mention I expect this formula to look something like -(target - output) * output * (1 - output) * inputs_from_previous_layer.  submitted by /u/Crealone [link] [comments] 