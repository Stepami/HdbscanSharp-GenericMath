loss vs iteration plot of a seq2seq model with a hidden size of 1
I have implemented a chatbot using a seq2seq model with online training (aka batch size = 1). I plotted my model's loss for a hidden layer size of 256 and saw a suspiciously nice plot (an elbow plot aka exponential decay). I further investigated by setting the hidden layer size to 1. To my surprise, I still saw an exponential decay in loss over iterations (granted, not as steep). Am I correct in saying that in a properly implemented model, this should not be possible; this is a clear indication of a bug in my code?  submitted by /u/yik_yak_paddy_wack [link] [comments] 