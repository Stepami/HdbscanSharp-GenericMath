[D] The decrease in PTB perplexity means zilch?
I am using SwitchBoard data for ASR. My ASR generated n-best output for a speech signal. I rerank the n-best output using Language Model probabilities and then calculate WER by comparing with reference text. I was mainly playing with LM. n-gram based language model gave 12.8% WER. Then I used simple RNN with variational dropout. It gave me 11.5% WER. I thought, not bad. Then I used SoTA LM (from Salesforce) open source code which has PTB perplexity of ~53. I tested on PTB, and I can replicate the result. On Switchboard data, my WER increased to 11.7%. I checked for four times. Edit: I am sorry. I am not clear. I trained the model on switch board transcript only. I used SoTA architecture. PTB improvement is not generalizing very well  [link] [comments] 