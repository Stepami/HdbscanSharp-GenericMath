/u/pavelchristof on Classification of each data point of long time series
100000+ is a lot, but the problem should be simple, so maybe with a small numbers of filters you could fit that into memory. I'd try approaches to segmentations based on dilated convolutions or an U-Net-like architecture: https://arxiv.org/pdf/1511.07122.pdf https://arxiv.org/pdf/1505.04597.pdf Yes, the values of the sensors can increase/decrease steadily across several hours but the fluctuations are limited by some underlying physical constraints.  It shouldn't be a problem. I was thinking about a trend like 0.01 * t or sin(t) * t, where no matter how much you zoom out the trend is still visible.   