/u/zacheism on Does it make sense to create polynomial features for use in a neural net?
Say you have three features in your dataset: A, B, C To create their polynomial features, you'd get the interaction between them, so for second order: A*B, A*C, B*C , A2 , B2 , C2 . You can increase the order as much as you want but the quantity of features obviously grows quite quick (and becomes pretty much useless, even with regularization), especially when working with high-dimension data to begin with. As AlexCoventry pointed out, a neural net should implicitly identify these relationships if they are important (to generalize, they will in one way or another be multiplied together in the first hidden layer). So I was wondering if it was even worth doing explicitly. In most linear models you can get a boost of performance from using these features because they aren't able to catch the non-linear relationship. Hope that helps! I'm testing it on a highly-tuned model w/ a fairly large dataset (~140k dimensions, second order, post-transformation) so I'd be surprised if I see much of a lift -- I'll update in about a week regardless :)   