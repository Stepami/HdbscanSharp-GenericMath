/u/deepvideoanalytics on [R] Semantic Image Synthesis
Oh come on synthesizing 64 X 64 (I couldn't find a single "large" image in the paper), blurry RGB "patches" with large number of artefacts in a very small constrained domain (flowers, birds) is barely "image synthesis". Why not just do Google Image Search and randomly crop or sample (or use YFCC 100M as source)? I completely fail to understand the point behind these "generation" papers? They seem like a poor way of learning image -> (parts,color) mapping. Except if you actually built a supervised models that mapped patches to part / color, no one would pay attention since its NOT a GAN! At some point someone has to call out this ridiculous trend. What's the point of BIRD-GANs, FLOWER-GANs, DISFIGURED-FACE-GANs? Let's be honest here the authors claim "intelligent image manipulation", looking at those images no sane Designer/User will ever use resulting images. If you are claiming to build something for "intelligent image manipulation", why not take a look at Graphics literature, and algorithms like PatchMatch or even Image Analogies [1] that actually deliver "intelligent image manipulation". The whole GAN literature these days looks like a sub-optimal solution in search of a problem. While conveniently ignoring all other existing approaches. The paper should be renamed as "GAN based encoder-decoder architecture for disentangling visual and textual semantics". [1] https://github.com/awentzonline/image-analogies   