model will not overfit training data
I can not seem to get my seq2seq model to overfit my training data even when I use a subset with only 3000 examples of length 10 and a vocab of 100. The model converges but not to a loss of zero and the value that it converges to seems to be correlated to the vocab size; a vocab size of 20,000 converges to a cross entropy loss of ~4 while a vocab size of 100 converges to ~2.5. My best guess is that the problem lies in the fact that I am using learnable embeddings as noted in this reddit post. But that doesn't seem plausible since the pytorch tutorial uses learnable embeddings but it manages to overfit just fine. Any suggestions? Some more details about the model: uses GRU cells with the context vector (encoder final hidden state) acting as init hidden state for the decoder no grad clipping unlike the SimpleDecoder in the pytorch seq2seq tutorial, there is no relu after my embedding; idk why they have one   submitted by /u/yik_yak_paddy_wack [link] [comments] 