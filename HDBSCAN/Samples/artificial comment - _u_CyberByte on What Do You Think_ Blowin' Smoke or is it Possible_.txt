/u/CyberByte on What Do You Think? Blowin' Smoke or is it Possible?
Maybe a little bit of both. There are definitely very real concerns about the potentially existential risks of artificial general intelligence (AGI). For more information on that, please see /r/ControlProblem and the information on its sidebar. Furthermore, there are very real concerns about the development and use of lethal autonomous weapons (LAWs), and vast amounts of AI researchers are against it and hope the UN will ban them (like they banned biological, chemical and to some degree nuclear weapons, as well as anti-personnel landmines). Both LAWs and AGI would be incredibly powerful technology, and many people fear an arms race. And to win an arms race, it can help to hobble your opponents. I could see how this might lead to war, but I feel like I really don't know enough politics and history for that. Similarly, I have no idea how volatile the situation with North Korea really is. Surely they know they can't win a nuclear conflict, but will that stop them from initiating one? Especially if they feel very threatened (maybe it will come to a point where some other nation decides NK's threat is too large, and they want to strike against the current regime). And if there is an armed conflict, will it be a World WarTM? Will it be global nuclear warfare? And exactly how much damage could NK do to the entire human race? In that sense I am inclined to believe that AGI is a much greater existential threat than just about anything if we don't solve the control problem. However, I don't really like that we constantly need to be comparing different threats to each other. "Yeah, sure, NK is dangerous, but what we should really be worrying about is AGI" or "yeah, sure, AGI may be dangerous one day, but what we should really be worrying about now is technological unemployment (or some other problem with narrow AI, or how bad people might use it)". How about we stop the pissing match and try to prevent/mitigate all of those problems?   