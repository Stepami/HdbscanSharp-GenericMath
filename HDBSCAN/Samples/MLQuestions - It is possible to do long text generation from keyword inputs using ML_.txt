It is possible to do long text generation from keyword inputs using ML?
Hi All! I'm really grateful this subreddit exists! I'm at a logistical logjam in my project and would love any input. Earlier this year, I played around with (Karpathy's RNN model in Tensorflow)[http://karpathy.github.io/2015/05/21/rnn-effectiveness/] and got really inspired. I trained it on primary school readers and got a really fun output. I took a dramatic leap and bite off a similar NLG project much more complicated (perhaps too complicated?) for school. I've accessed (the Linguistic Data Consortium's NYT Digital Corpus)[https://catalog.ldc.upenn.edu/ldc2008t19] which has 1.5m+ articles from 1986 to 2006 coded in an XML format using (NITF standard)[https://iptc.org/standards/nitf/]. Each article has metadata like keywords, publication date, taxonomic classification (news, arts, etc) as well as the full text of the article, including the lead paragraph and occasionally an archival abstract. I wrote a script in Python that takes the corpus and pulls out articles that meet the criterion of having "Top/News/Russia" in their taxonomic classification. I got close to 9k articles, which I piped into R for clean-up. After much hair-pulling and stackoverflow-ing, I've finally created a corpus object in R that contains each article as a document with its respective metadata points encoded as corpus metadata. Now, I'd like to train a network using some of the metadata points as input and the article text as expected output. I'm at a bit of a loss to know what kind of network (or combination of networks) to use. I've taken the Coursera course with Andrew Ng and am fairly competent in Python and R. I've been looking through the notebooks and videos for (fast.ai)[www.fast.ai/] and I feel like I'm getting closer, but I still feel confused. Anyone have any thoughts or feedback? (Advice about the quixotic nature of this project is duly noted in advance and not necessary to repeat here as it runs through my head everyday for at least an hour) Edited: formatting  submitted by /u/cooganb [link] [comments] 