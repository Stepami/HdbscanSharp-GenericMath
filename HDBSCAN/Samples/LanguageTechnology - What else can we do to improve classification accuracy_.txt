What else can we do to improve classification accuracy?
I come into a problem where there are certain features (words) that appear in all the classes and this seems to make my classifier confused on classifying the documents. As a result, when I use certain classifiers (Multinomial Naive Bayes, SVM, etc) the classifier fails to predict the right classes of the documents and instead it classifies all the documents to one certain class (which is very dominant in my case, it's quite imbalance dataset). Strangely, when I use something really simple like KNN, it can predict better (from the confusion matrix, I know all the data are no longer classified to one certain class anymore), albeit still low in accuracy (around 60%). I figure there must be some ways you can do to improve accuracy? N-grams, I did bigrams and trigrams to my classifier, it doesn't improve the accuracy much.  Eliminating terms that only appear less than certain number in the documents. I limit it to min_df=5 and it reduces a great portion of features, from like 100k to only 2k, but with similar accuracy. I am quite impressed by this.   What else can I do? I am actually thinking of something like feature selection (information gain, chi-square) but I haven't found any tutorial detailing about this on python. And I have no idea where to begin. Could anyone here suggest me some other methods? Thanks!  submitted by /u/eggman-or-walrus [link] [comments] 