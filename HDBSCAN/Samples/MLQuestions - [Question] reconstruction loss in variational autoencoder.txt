[Question] reconstruction loss in variational autoencoder
I can understand the KL part in the lower bound. For Bernoulli decoder, the reconstruction loss for one sample is log p(x|z) = x log(f(z)) + (1-x) log(1-f(z)), where f is neural network that transforms z into the parameter of Bernoulli distribution  In many implementations I found, like this one and this one. They first scale the mnist data into a float number between 0 and 1, however, for Bernoulli likelihood function, the data x should be a binary number, that is 0 or 1, not a float in [0, 1]. From my understanding, the correct implementation should use the binary mnist, not the real valued mnist. Can anyone explain this to me? Besides, the sigmoid output should be the parameters of the Bernoulli, when generating one sample, there should be a sampling process, but in these implementations, they just use the sigmoid output as the generated image. Can anyone explain this to me?  submitted by /u/maxc01 [link] [comments] 