Why overfitting does not happen?
I am solving a classification problem on a pretty big dataset (millions of samples). What is strange is that different models (CNN, MLP, RNN) with different number of parameters ( from ~3k to 3kk) perform similarly. Bigger models do not overfit, learning curves (loss) look similar. The auc is decent, so the model is not underfitting. What is going on? Is the problem too simple? And, in general, when increasing capacity of a neural network and not using regularization why overfitting might not happen? Thanks!  submitted by /u/g3n1uss [link] [comments] 