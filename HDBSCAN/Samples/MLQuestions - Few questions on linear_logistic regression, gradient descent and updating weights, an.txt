Few questions on linear/logistic regression, gradient descent and updating weights, and neural networks
For my own background, I've taken several undergraduate machine learning and AI courses, two calculus classes, and majored in Computer Science. I did pretty good in coursework, am really trying to perfectly understand some concepts... questions are as follows: 1) Gradient descent with multiple features From what I've learned about regression, there is some quadratic cost function (the sum of the squared errors of each sample) to be minimized using gradient descent. A straight line is created with a weight for each feature + a bias. For a problem with multiple features, when doing the gradient update how is it determined which feature (edit: feature weight) is updated? Are they all updated at once, in a linear fashion, based on some criteria, ...? I never took 3D Calc and my professors would always talk about taking the "partial derivative", which I never really understood. This leads to my next question: 2) Creating a model where the number of features > number of samples Is it true that if the number of features <= number of samples, then there will be only one truly minimized solution/combination of feature weights for the model? I remember something that for linear/logistic regression, the optimization problem can be treated as solving a system of linear equations, where the number of equations is equal to the number of samples, and the number of unknown variables is equal to the number of features. So then whenever the number of features is greater than the number of samples the possible values for the features is then infinite, and if there are even more features then the chance for overfit is even greater. This also leads to my next question: 3) Regression line as a higher order polynomial I was looking at some YouTube videos and they were saying that the regression line can be transformed to a higher order polynomial, and that this transformation does not really change the inherent way the optimization is done. However, how can it be determined which feature weight is assigned whatever degree polynomial? This seems like a different problem to me than a simple linear case, since wouldn't many combinations of different degree polynomials have to be tried on each individual feature? 4) Logistic regression vs. neural network Is a one-layer neural network literally the same thing as logistic regression with a linear boundary? I was under the impression that for each additional layer, it adds another degree to the polynomial of the decision boundary eg. a two-layer NN is quadratic? I think this question may be cleared up if the other three are answered though. Thank you.  submitted by /u/bloosnail [link] [comments] 