Feature Importances of Cross-Validated Model?
Is there any way to get at the feature importances of a cross-validated model? Here's my cross validation  kfold = model_selection.KFold(n_splits=25, random_state=seed) cart = RandomForestClassifier() num_trees = 1000 model = RandomForestClassifier(n_estimators=num_trees, random_state=seed) results = model_selection.cross_val_score(model, X, Y, cv=kfold) print(results.mean())  Which performs with ~.73 accuracy. I'd like to get the feature importances behind that score, but instead, the closest I can seem to get is  results=model.fit(X,Y) importances = model.feature_importances_ print(importances)  If I'm not mistaken, because I had to refit, that's a totally different animal. The average cross-validated score is 73% accuracy, but any given random forest could perform at 85% or 60% or 3%, right? Furthermore, there are 50 features, and the importances don't get much higher than 2%. So the importance is spread evenly, ergo probably randomly between them, right? But when I ran them through a t-test, half of my features have p-values < .05. Does it seem like something went wrong? Finally, my general plan here is to create a few datasets with different importance cutoffs (eliminating the features that don't meet that bar), run them through some parameter tuned cross validation, and see what set of features performs the best. Is that likely to cause overfitting? Or any other problems down the road? Any ideas would be appreciated. I'm going a little cross-eyed trying to figure this out. Thanks a lot.  [link] [comments] 