/u/CyberByte on Artificial Life as the path to AGI.
I think intrinsic rewards are very interesting, but I wouldn't really like to have an AGI with curiosity as its sole reward function. Systems that are already very intelligent should probably realize that (basic AI) drives for knowledge and survival (among other things) are instrumental to its top-level goals / reward function (pretty much no matter they are). A "baby" AGI might not realize this yet, and so it might be useful to start it out with some innate curiosity, but I think the same could be said for other things like a drive for survival. Also, it seems nice if there was a drive for something that's actually useful (or even just safe) to us, especially if the AGI keeps getting more and more intelligent. I think I'm sort of coming around to Stuart Russell's idea that perhaps we shouldn't give our AGIs definitive objective functions at all. Instead, they should have uncertainty over the true objective function and use some sort of cooperative inverse reinforcement learning to learn from human interactions what is good and bad. I think there are still many challenges to getting this to work, but it seems like it might be a potential way to get human-compatible (or provably beneficial) AGI as he calls it. But even if we do decide that Schmidhuber's curiosity is the right reward function, it still seems like the main challenge is to build an architecture for optimizing reward functions like that...   