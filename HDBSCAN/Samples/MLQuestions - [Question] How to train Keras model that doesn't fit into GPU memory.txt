[Question] How to train Keras model that doesn't fit into GPU memory
Using TF back end, but not tied to it. Right now, in order to train the model, I have to reduce batch size to 3 so that the whole thing fits into GPU memory. The model is very fragile during training- I suspect it's experiencing brain death, and I want to try batch-norm to counter that. But, batch-norm has big problems with very small batches. I want to get my batches up to at least 8. I could get another GPU with 24GB but even then I wouldn't get batch sizes higher than 6 or 7. Accepting the performance hit, surely there must be a way to work with models that don't fit entirely into GPU memory, without going to a lower-level api right?  submitted by /u/karn1948 [link] [comments] 