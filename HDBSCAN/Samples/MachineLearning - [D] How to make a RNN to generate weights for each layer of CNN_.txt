[D] How to make a RNN to generate weights for each layer of CNN?
This may be late, but I'm trying to implement RNN version of SMASH (hypernet being RNN) with some modifications. Let c be a binary vector describing a CNN architecture. The candidate CNN may have variable layer size, activation function, filter width/height and etc, and the dimension of c is about 2400. Each input vector has dimension about 12 in the way compatible with the encoding rule, and the input "sentence" consists of 200 input vectors. Since the output dimension is different from that of the input, encoder-decoder (w/ attention) is needed, and I prefer QRNN or SRU to be used. The issue is the following: since each layer has different size (# of weights), and since each candidate architecture has different depth and n-th layer size, there is no canonical way to map each output vector to weights of the layer. What would be a good way to do it? How do I implement the process of substituting output vectors {o_i} to weights of CNN c, as this kind of task, as far as I know, isn't commonly done? One way I can think of is to use a single MLP/CNN to map each output vector to weights of the corresponding layer, and another way is to naively map the elements of the output vector to weights of the corresponding layer and discard the excessive last elements of the vector. However, I'm sure there are better ways, and I have no idea how to make them more concrete. The reason why I'm not using memory bank is to make the algorithm more amenable to replacing the network to be searched for from CNN to RNN and other kinds.  submitted by /u/HigherTopoi [link] [comments] 