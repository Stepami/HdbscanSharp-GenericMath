General advice on choosing architecture for char-RNN applied to Dota drafting
thinking about text generation, a draft of dota2 (teams picking/banning 5 heroes each out of total 113), is analogous to char-RNN. the vocab size is just 113, number of possible heroes, and every word/draft is simply 20 characters long. getting to the point: my keras model is 3 layers of lstm nodes with some dropout between each. I need some confirmation as to whether this is a sensible starting point. if so, what would sensible starting node sizes in each layer and dropouts be  model = Sequential() if dropout1: model.add(Dropout(dropout1, input_shape=(None, VOCAB_SIZE))) model.add(LSTM(nodes1, input_shape=(None, VOCAB_SIZE), return_sequences=True)) if dropout2: model.add(Dropout(dropout2)) model.add(LSTM(nodes2, return_sequences=True)) if dropout3: model.add(Dropout(dropout3)) if nodes3: model.add(LSTM(nodes3, return_sequences=True)) model.add(TimeDistributed(Dense(VOCAB_SIZE))) model.add(Activation('softmax')) optimizer = adam(lr=learning_rate) model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['mse', 'accuracy'])  Whilst I am plotting learning curves, checking overfitting and tweaking based on that. I am very un-experienced in machine learning. Any advice/helpful suggestions about how many layers I should be starting with, how many nodes in each layer, how much dropout....as a solid starting point that I can iterate on....would be much appreciated for above I had (Train on 6726 sequences, validate on 1681 sequences...but I can increase this number to ~50,000. thinking a deep net will be necessary for what I want it to learn...and way more than 6700 sequences will be needed to not overfit) extra info: One thing I wanted to see was whether a neural net could figure out that you could only ban/pick a hero once in a sequence, without overfitting to just copy drafts. It nearly gets the hang of it. this is the point where validation accuracy peaks then starts to drop off  26s - loss: 3.6208 - mean_squared_error: 0.0085 - acc: 0.1276 - val_loss: 3.9178 - val_mean_squared_error: 0.0086 - val_acc: 0.0906 Pick: Sand King, Crystal Maiden, Rubick, Queen of Pain, Ursa VS Crystal Maiden, Clockwerk, Disruptor, Lifestealer, Tinker Ban: Broodmother, Treant Protector, Juggernaut, Invoker, Storm Spirit VS Bristleback, Night Stalker, Shadow Fiend, Juggernaut, Sven  (was with 200, 200, 150 for nodes in each lstm layer. 0.3, 0.2, 0 for dropout between layers) I can probably do better though, issue Im having is that 131 possible chars is a lot, 20 letter words are quite long as well. I feel like I need to be using a pretty deep net to achieve my code: https://pastebin.com/yd59bbP2 note: a lot of code was from following this https://chunml.github.io/ChunML.github.io/project/Creating-Text-Generator-Using-Recurrent-Neural-Network/ and so isnt really my work in a way.  submitted by /u/LePianoDentist [link] [comments] 