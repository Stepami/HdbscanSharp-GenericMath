/u/CyberByte on I just thought of something. If a super-intelligent AI valued self-preservation, it may not change anything to make everyone oblivious of its existence.
On the other hand, this ASI would be ceding power and ultimately control over its destiny, not just to hypothetical entities that are more powerful at the moment, but also to any and all entities/ASIs that will now surpass it in power. While the existence, motives and exact capabilities of alien entities are fairly unknown, the ASI can be relatively certain that if it was developed by humanity and it is somewhat underwhelming, humanity will endeavor to develop more and better versions. The ASI may not get killed because it's a threat, but it might get killed because it's useless or collateral damage in the plans of some more powerful entity that just doesn't care about the ASI. Perhaps even an entity that the ASI could have squashed if it hadn't self-handicapped so much. And all this time it's paying potentially massive opportunity costs on the goals it actually cares about. I agree with much of what you've said, and there could be good reasons for an ASI to want to stay hidden. I don't want to dismiss that possibility. But I can equally well see reasons for not doing that. It will come down to a risk/benefit analysis made by an entity way smarter than you or me.   