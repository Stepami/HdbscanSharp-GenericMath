Just started my first Kaggle competition, and I have several questions
After reading as theory, I attempted my first Kaggle dataset and it raised several question for me: There are many things I can do that give relatively small improvement, such as changing several feature with class to continues numeric values. Or some other changes such as feature engineering and so on. The problem with these changes is that their improvement is so small, I can't judge using my own train data whether they contributed or not (the randomness in algorithm alone has a larger noise). So how do I approach these things? Do I just guess they would help? Or maybe there's another method? I'm using XGBOOST. In some guide I read it was recommended to not use more than 1000 trees. I was wondering - why is that? Is it to prevent overfitting or to save on runtime? There's this basic idea of picking the model not with the best validation score, but with a high validation score is high but not so much that the training score is much higher than it. So I just need to clarify to myself this concept:Did I understand it correctly? The reason for that is to prevent a case of overfitting - that is, that the model would work well on unseen data, right?    Thanks!  submitted by /u/Justin7711233 [link] [comments] 