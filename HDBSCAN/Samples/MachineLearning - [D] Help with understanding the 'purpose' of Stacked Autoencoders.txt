[D] Help with understanding the 'purpose' of Stacked Autoencoders
I think I'm close to understanding the point of Stacked Autoencoders, but I need a little bit more of a nudge. Here's what I (questionably) know: Functionally, Autoencoders try to closely recreate their input. For example, training an Autoencoder on MNIST (28x28=784) will produce a new image (28x28=784) that looks at least somewhat similar.  I can pass my own personal handwriting of 0 into the Autoencoder. From there, I could care about one of two possibilites (3 or 4).  First possibility: I pass my handwriting 0 through the Encoder as a 'Dimension Reduction' tool. I can use the reduced image (5x5=25) as input to a classifier to convert my handwriting digit into Int32.Zero.  Second possibility: I pass my handwriting 0 through the Encoder, and when it gets Decoded, it returns the MNIST sample that my handwriting is most similar to.  Referring to this link: https://www.toptal.com/machine-learning/an-introduction-to-deep-learning-from-perceptrons-to-deep-networks What is the +1 Neuron?   Are these applications correct? Are there any applications for stacked Autoencoders that I'm missing?  submitted by /u/virivim [link] [comments] 