/u/salacious_sackbut on [D] [Question] Rademacher complexity and VC dimension
The generalization error of a model can be (PAC)-bounded above by two terms: (1) the error that the model makes on a fixed training set drawn from the data distribution (the "empirical error"), and (2) a measure of the model's "capacity". Decreasing the capacity of the model generally increases the first term and decreases the second term. On the other hand, increasing the capacity of the model generally decreases the first term, and increases the second term. When the generalization error of a model suffers because its capacity is made too small, we say that the model is underfitting. And when the generalization error suffers because the capacity is made too large, we say that the model is overfitting. One of the main ideas of learning theory is that learning is a tradeoff between minimizing the sum of these two terms. Now, when we come to studying a specific family of models (e.g. SVMs), we might want to prove that the family is PAC-learnable. This is done by establishing a PAC-bound of the kind discussed in the previous paragraph. To establish a PAC-bound, we need a suitable notion for capacity: for this, we use the growth function. The growth function bounds both the VC-dimension and the Rademacher complexity from above. Hence, to establish a PAC-bound for a model family, it suffices to show that either its VC-dimension or its Rademacher complexity is finite. It turns out to be much easier to bound the Rademacher complexity of a family of models than its VC-dimension. Many of the proofs in texts on learning theory were greatly simplified after the introduction of Rademacher complexity. To my knowledge, this is the role that Rademacher complexity plays in learning theory: it provides a convenient route for proving that a function family is PAC-learnable.   